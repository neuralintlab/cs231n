{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from statistics import mode\n",
    "\n",
    "DATA_PATH = '/home/youngmin/Dataset/cifar-10-batches-py/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_name):\n",
    "    with open(DATA_PATH + file_name, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='latin1')\n",
    "        \n",
    "    return dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'num_cases_per_batch': 10000,\n",
       " 'label_names': ['airplane',\n",
       "  'automobile',\n",
       "  'bird',\n",
       "  'cat',\n",
       "  'deer',\n",
       "  'dog',\n",
       "  'frog',\n",
       "  'horse',\n",
       "  'ship',\n",
       "  'truck'],\n",
       " 'num_vis': 3072}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# meta data\n",
    "batches_meta = load_data('batches.meta')\n",
    "batches_meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type of data for batch_label=<class 'str'>, len=21\n",
      "type of data for labels=<class 'list'>, len=10000\n",
      "type of data for data=<class 'numpy.ndarray'>, len=10000\n",
      "type of data for filenames=<class 'list'>, len=10000\n"
     ]
    }
   ],
   "source": [
    "# structure of a batch file\n",
    "batch = load_data('data_batch_1')\n",
    "\n",
    "for key, value in batch.items():\n",
    "    print(f'type of data for {key}={type(value)}, len={len(value)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4 5 6 7 8 9]\n",
      "<class 'int'>\n"
     ]
    }
   ],
   "source": [
    "# print unique labels and data type\n",
    "print(np.unique(batch['labels']))\n",
    "print(type(batch['labels'][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data file tuple\n",
    "data_list_training = ('data_batch_1', 'data_batch_2', 'data_batch_3', 'data_batch_4', 'data_batch_5')\n",
    "data_test = 'test_batch'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load and check training data\n",
    "Xtr = []\n",
    "Ytr = []\n",
    "\n",
    "for batch_name in data_list_training:\n",
    "    batch = load_data(batch_name)\n",
    "    Xtr.append(batch['data'])\n",
    "    Ytr.append(batch['labels'])\n",
    "\n",
    "Xtr = np.concatenate(Xtr)\n",
    "Ytr = np.concatenate(Ytr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 3072)\n",
      "(50000,)\n"
     ]
    }
   ],
   "source": [
    "print(Xtr.shape)\n",
    "print(Ytr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 5000, 1: 5000, 2: 5000, 3: 5000, 4: 5000, 5: 5000, 6: 5000, 7: 5000, 8: 5000, 9: 5000}\n"
     ]
    }
   ],
   "source": [
    "labels, counts = np.unique(Ytr, return_counts=True)\n",
    "print({label:count for label, count in zip(labels, counts)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 3072)\n",
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "# load and check test data\n",
    "batch = load_data(data_test)\n",
    "Xte = batch['data']\n",
    "Yte = np.array(batch['labels'])\n",
    "print(Xte.shape)\n",
    "print(Yte.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set\n",
      "global mean = -0.02664\n",
      "global std  = 0.25157\n",
      "global max = 0.50000\n",
      "global min = -0.50000\n",
      "\n",
      "Test set\n",
      "global mean = -0.02342\n",
      "global std  = 0.25122\n",
      "global max = 0.50000\n",
      "global min = -0.50000\n"
     ]
    }
   ],
   "source": [
    "# normalize data (run just once!)\n",
    "Xtr = (Xtr - 127.5) / 255\n",
    "Xte = (Xte - 127.5) / 255\n",
    "\n",
    "# check normalized data\n",
    "print('Training set')\n",
    "print('global mean = %.5f' % np.mean(Xtr))\n",
    "print('global std  = %.5f' % np.std(Xtr))\n",
    "print('global max = %.5f' % np.max(Xtr))\n",
    "print('global min = %.5f' % np.min(Xtr))\n",
    "\n",
    "print()\n",
    "print('Test set')\n",
    "print('global mean = %.5f' % np.mean(Xte))\n",
    "print('global std  = %.5f' % np.std(Xte))\n",
    "print('global max = %.5f' % np.max(Xte))\n",
    "print('global min = %.5f' % np.min(Xte))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Forward Pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"cs231n_slide_001.png\" width=\"600\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our dimensions\n",
    "* a mini-batch of images, x: (batch_size, n_dim) = (batch_size, 3072)\n",
    "* weight matrix, W: (n_dim, n_class) = (3072, 10)\n",
    "* bias, b: (n_class) = (10)\n",
    "* output: np.dot(x, W) + b, shape = (batch_size, 3072) â€¢ (3072, 10) + (10) = (batch_size, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# softmax function (stable ver)\n",
    "def softmax(y): # input y is output of the model y = xW + b\n",
    "    y = y - np.max(y, axis=1)[:, np.newaxis] # maximum output would be 0, np.exp(max) = 1\n",
    "    denominator = np.sum(np.exp(y), axis=1)[:, np.newaxis]\n",
    "    return np.exp(y) / denominator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# linear classifer class with softmax output\n",
    "class LinearClassifier(object):\n",
    "\n",
    "    # initialize parameters\n",
    "    def __init__(self, n_dim, n_class):\n",
    "        # n_dim = number of pixels in image (3072 in CIFAR-10 image)\n",
    "        # n_class = number of classes to classify (10 in CIFAR-10)\n",
    "        self.W = 0.01 * np.random.randn(n_dim, n_class)  # initial random weight: W.shape = (10, 3072)\n",
    "        self.b = np.zeros(n_class) # initial zero bias: b.shape = (10)\n",
    "\n",
    "    # forward-pass: giving softmax scores for each class\n",
    "    def forward(self, x): # x is a mini-batch of images: x.shape = (batch_size, n_dim)\n",
    "        y = np.dot(x, self.W) + self.b\n",
    "        return softmax(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of W = (3072, 10)\n",
      "shape of b = (10,)\n"
     ]
    }
   ],
   "source": [
    "# make an instance of linear classifier\n",
    "n_dim = Xtr.shape[1]\n",
    "n_class = 10\n",
    "\n",
    "model = LinearClassifier(n_dim, n_class)\n",
    "\n",
    "print('shape of W =', model.W.shape)\n",
    "print('shape of b =', model.b.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-hot encoder\n",
    "def onehot(y, n_class):\n",
    "    vectors = np.zeros((len(y), n_class))\n",
    "    for i, label in enumerate(y):\n",
    "        vectors[i, label] = 1\n",
    "    return vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross-entropy loss for all data in a batch (single number)\n",
    "def loss(y_true, y_pred):\n",
    "    return np.mean(np.sum(-y_true * np.log(y_pred), axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss = 2.31405\n"
     ]
    }
   ],
   "source": [
    "# calculate loss for the entire test set\n",
    "y_pred = model.forward(Xte)\n",
    "y_true = onehot(Yte, n_class)\n",
    "test_loss = loss(y_true, y_pred)\n",
    "print('Test loss = %.5f' % test_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Backward Pass: Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://cs231n.github.io/assets/dataflow.jpeg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.31107876512688e+00\n",
      "2.316581109853437e+00\n",
      "2.323075277359668e+00\n",
      "2.3191626529801557e+00\n",
      "2.306908294424832e+00\n",
      "2.3260511999513254e+00\n",
      "2.3102723663597406e+00\n",
      "2.3072708035161917e+00\n",
      "2.316151118944066e+00\n",
      "2.305181914875923e+00\n",
      "2.307220222511729e+00\n",
      "2.3180498348710117e+00\n",
      "2.3074290315326556e+00\n",
      "2.314697103509052e+00\n",
      "2.3215944285756693e+00\n",
      "2.3149922073054574e+00\n",
      "2.3171551217700173e+00\n",
      "2.323346221739264e+00\n",
      "2.3088405328673507e+00\n",
      "2.3071219873310866e+00\n",
      "2.303668580158443e+00\n",
      "2.3180380591964047e+00\n",
      "2.3112865633597623e+00\n",
      "2.303320829737117e+00\n",
      "2.323088564679035e+00\n",
      "2.3218918254463157e+00\n",
      "2.3129419172757606e+00\n",
      "2.312106604605332e+00\n",
      "2.3136913988833245e+00\n",
      "2.320141270563785e+00\n",
      "2.3073063634541793e+00\n",
      "2.305819858907767e+00\n",
      "2.311366887724976e+00\n",
      "2.313322335881469e+00\n",
      "2.324444446755415e+00\n",
      "2.318380221570942e+00\n",
      "2.307915877719276e+00\n",
      "2.3163286486575863e+00\n",
      "2.311735641529483e+00\n",
      "2.3237585123705347e+00\n",
      "2.3145986141714747e+00\n",
      "2.31191179764614e+00\n",
      "2.3136637628916223e+00\n",
      "2.313483889774948e+00\n",
      "2.3111895435168344e+00\n",
      "2.2943399870633074e+00\n",
      "2.316634498848628e+00\n",
      "2.3136646595586328e+00\n",
      "2.318151944949428e+00\n",
      "2.3214403285112026e+00\n",
      "2.303908407138345e+00\n",
      "2.31342687802743e+00\n",
      "2.3115776490562974e+00\n",
      "2.3198538589015265e+00\n",
      "2.3115868919587386e+00\n",
      "2.3149259732263854e+00\n",
      "2.3113460200135023e+00\n",
      "2.312314989212908e+00\n",
      "2.3109048048278145e+00\n",
      "2.3233619804075487e+00\n",
      "2.312027179425926e+00\n",
      "2.3047305682017565e+00\n",
      "2.306310694778009e+00\n",
      "2.309747340239671e+00\n",
      "2.3171307979858193e+00\n",
      "2.316172115685549e+00\n",
      "2.30851573134967e+00\n",
      "2.3189732042945796e+00\n",
      "2.3230209442674115e+00\n",
      "2.303524235640899e+00\n",
      "2.305471825625042e+00\n",
      "2.321784089409808e+00\n",
      "2.317296211439482e+00\n",
      "2.314648008851039e+00\n",
      "2.3082257546725633e+00\n",
      "2.334767929089245e+00\n",
      "2.314397267264456e+00\n",
      "2.3141973902644954e+00\n",
      "2.3090278694006483e+00\n",
      "2.305640007407388e+00\n",
      "2.30126319267706e+00\n",
      "2.320403150338195e+00\n",
      "2.3126962397233983e+00\n",
      "2.318198490876401e+00\n",
      "2.322250505052695e+00\n",
      "2.3203509946786203e+00\n",
      "2.3266298719505465e+00\n",
      "2.3163551567837386e+00\n",
      "2.3253841634765604e+00\n",
      "2.3090390780916414e+00\n",
      "2.326392265744215e+00\n",
      "2.3137732319859134e+00\n",
      "2.3177322779081666e+00\n",
      "2.30975668508074e+00\n",
      "2.3138414318070946e+00\n",
      "2.3077147042146158e+00\n",
      "2.31179916964095e+00\n",
      "2.3221761083690127e+00\n",
      "2.32415829368867e+00\n",
      "2.2956741905172886e+00\n",
      "2.3001880931841314e+00\n",
      "2.3282381933313054e+00\n",
      "2.2927707507472634e+00\n",
      "2.313416406709243e+00\n",
      "2.3183588018883894e+00\n",
      "2.315969141505675e+00\n",
      "2.310314936045076e+00\n",
      "2.3216445845831193e+00\n",
      "2.311983038278204e+00\n",
      "2.2970454089512122e+00\n",
      "2.3117967862966884e+00\n",
      "2.3187679464883875e+00\n",
      "2.3027402497285636e+00\n",
      "2.2985838668765615e+00\n",
      "2.299668798390826e+00\n",
      "2.325982183546128e+00\n",
      "2.3145986517777852e+00\n",
      "2.313494187021771e+00\n",
      "2.3238208976523174e+00\n",
      "2.3216485479326923e+00\n",
      "2.3233216840972806e+00\n",
      "2.3204695023442095e+00\n",
      "2.3035847900554782e+00\n",
      "2.3033740827889937e+00\n",
      "2.3213221777337356e+00\n",
      "2.3040949052401967e+00\n",
      "2.3069610651121057e+00\n",
      "2.318179680744069e+00\n",
      "2.303695357158049e+00\n",
      "2.3154824223564985e+00\n",
      "2.3144805940161874e+00\n",
      "2.316089690120073e+00\n",
      "2.325264816481181e+00\n",
      "2.3069204287651934e+00\n",
      "2.3169744070806173e+00\n",
      "2.3135061637369865e+00\n",
      "2.3081258542212337e+00\n",
      "2.309161701952742e+00\n",
      "2.3123310221185074e+00\n",
      "2.31543218153463e+00\n",
      "2.2976759682592953e+00\n",
      "2.320759716544282e+00\n",
      "2.311404299316275e+00\n",
      "2.3075252235853996e+00\n",
      "2.3292735623986847e+00\n",
      "2.310275921599571e+00\n",
      "2.3202277092383556e+00\n",
      "2.308625599762771e+00\n",
      "2.309697720492545e+00\n",
      "2.3206959562646854e+00\n",
      "2.3108216007139246e+00\n",
      "2.3034957670844287e+00\n",
      "2.3006876065396846e+00\n",
      "2.3082333091624823e+00\n",
      "2.3057216099231534e+00\n",
      "2.3257235522396145e+00\n",
      "2.30940744788464e+00\n",
      "2.3136221483043604e+00\n",
      "2.31747976012227e+00\n",
      "2.310465648344429e+00\n",
      "2.3112980585764125e+00\n",
      "2.3194440321688314e+00\n",
      "2.3045886741657124e+00\n",
      "2.31265680235759e+00\n",
      "2.3213647976202485e+00\n",
      "2.310146140243329e+00\n",
      "2.3168239352972653e+00\n",
      "2.310513431969265e+00\n",
      "2.3102086518044653e+00\n",
      "2.3167180590734624e+00\n",
      "2.3054445563753623e+00\n",
      "2.3162291302698894e+00\n",
      "2.3061393119138938e+00\n",
      "2.316469644249184e+00\n",
      "2.319197161428866e+00\n",
      "2.323546541992244e+00\n",
      "2.3118799923264337e+00\n",
      "2.3157432979374946e+00\n",
      "2.315157922988687e+00\n",
      "2.313446269687809e+00\n",
      "2.3253573799622815e+00\n",
      "2.3142181838046265e+00\n",
      "2.314522263925979e+00\n",
      "2.3228616565197355e+00\n",
      "2.3053804948837913e+00\n",
      "2.29784072021423e+00\n",
      "2.309020397411126e+00\n",
      "2.3094124331994914e+00\n",
      "2.3169904945825692e+00\n",
      "2.310533021280177e+00\n",
      "2.312778951396339e+00\n",
      "2.3163421627868965e+00\n",
      "2.320234156674225e+00\n",
      "2.3015990200340837e+00\n",
      "2.3199088147910403e+00\n",
      "2.2946349377928725e+00\n",
      "2.3183793022816372e+00\n",
      "2.322350690771222e+00\n",
      "2.3110975290455e+00\n"
     ]
    }
   ],
   "source": [
    "# code block to iterate over training data with mini-batches\n",
    "# print batch-loss for each batch\n",
    "batch_size = 250\n",
    "n_samples = Xtr.shape[0]\n",
    "\n",
    "idx = 0\n",
    "while True:\n",
    "    if idx + batch_size >= n_samples:\n",
    "        break\n",
    "\n",
    "    batch_X = Xtr[idx: idx + batch_size]\n",
    "    batch_Y = Ytr[idx: idx + batch_size]\n",
    "\n",
    "    Y_pred = model.forward(batch_X)\n",
    "\n",
    "    Y_true = onehot(batch_Y, n_class)\n",
    "    batch_loss = loss(Y_true, Y_pred)\n",
    "    print(batch_loss)\n",
    "\n",
    "    idx += batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](eq_gradient.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_gradW(model, loss_fn, X_batch, Y_batch):\n",
    "    Y_pred = model.forward(X_batch)\n",
    "    f = loss_fn(Y_batch, Y_pred)\n",
    "    grad_vec = np.zeros(model.W.size)\n",
    "    h = 0.001\n",
    "\n",
    "    # iterate over all parameters in W\n",
    "    for i in range(model.W.size):\n",
    "        dW_vec = np.zeros(model.W.size)\n",
    "        dW_vec[i] = h # only i-th parameter increases by h (others 0)\n",
    "        dW = dW_vec.reshape(model.W.shape)\n",
    "\n",
    "        model.W += dW\n",
    "        Y_pred = model.forward(X_batch)\n",
    "        fh = loss_fn(Y_batch, Y_pred)\n",
    "        grad_vec[i] = (fh - f) / h\n",
    "        model.W -= dW\n",
    "\n",
    "    grad = grad_vec.reshape(model.W.shape)\n",
    "\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_gradb(model, loss_fn, X_batch, Y_batch):\n",
    "    Y_pred = model.forward(X_batch)\n",
    "    f = loss_fn(Y_batch, Y_pred)\n",
    "    grad_vec = np.zeros(model.b.size)\n",
    "    h = 0.001\n",
    "\n",
    "    # iterate over all parameters in W\n",
    "    for i in range(model.b.size):\n",
    "        db_vec = np.zeros(model.b.size)\n",
    "        db_vec[i] = h # only i-th parameter increases by h (others 0)\n",
    "        db = db_vec.reshape(model.b.shape)\n",
    "\n",
    "        model.b += db\n",
    "        Y_pred = model.forward(X_batch)\n",
    "        fh = loss_fn(Y_batch, Y_pred)\n",
    "        grad_vec[i] = (fh - f) / h\n",
    "        model.b -= db\n",
    "\n",
    "    grad = grad_vec.reshape(model.b.shape)\n",
    "\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross-entropy loss for all data in a batch (single number)\n",
    "def CEloss(y_true, y_pred):\n",
    "    return np.mean(np.sum(-y_true * np.log(y_pred), axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(y_true, y_pred):\n",
    "    y_pred_class = np.argmax(y_pred, axis=1)\n",
    "    return np.count_nonzero(y_true == y_pred_class) / len(y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1\n",
      "batch 1: running loss = 2.30539, running accuracy = 0.08400\n",
      "batch 2: running loss = 2.27261, running accuracy = 0.14400\n",
      "batch 3: running loss = 2.23743, running accuracy = 0.17600\n",
      "batch 4: running loss = 2.22342, running accuracy = 0.18200\n",
      "batch 5: running loss = 2.20612, running accuracy = 0.19360\n",
      "batch 6: running loss = 2.18720, running accuracy = 0.20067\n",
      "batch 7: running loss = 2.17348, running accuracy = 0.21429\n",
      "batch 8: running loss = 2.17417, running accuracy = 0.21300\n",
      "batch 9: running loss = 2.16743, running accuracy = 0.21467\n",
      "batch 10: running loss = 2.15526, running accuracy = 0.22160\n",
      "batch 11: running loss = 2.14776, running accuracy = 0.22400\n",
      "batch 12: running loss = 2.13953, running accuracy = 0.22867\n",
      "batch 13: running loss = 2.13438, running accuracy = 0.23354\n",
      "batch 14: running loss = 2.12777, running accuracy = 0.23457\n",
      "batch 15: running loss = 2.11814, running accuracy = 0.24267\n",
      "batch 16: running loss = 2.11339, running accuracy = 0.24825\n",
      "batch 17: running loss = 2.10668, running accuracy = 0.25035\n",
      "batch 18: running loss = 2.10433, running accuracy = 0.25111\n",
      "batch 19: running loss = 2.09798, running accuracy = 0.25684\n",
      "batch 20: running loss = 2.09553, running accuracy = 0.25880\n",
      "batch 21: running loss = 2.09409, running accuracy = 0.26038\n",
      "batch 22: running loss = 2.08833, running accuracy = 0.26455\n",
      "batch 23: running loss = 2.08288, running accuracy = 0.26748\n",
      "batch 24: running loss = 2.07790, running accuracy = 0.26983\n",
      "batch 25: running loss = 2.07236, running accuracy = 0.27152\n",
      "batch 26: running loss = 2.06959, running accuracy = 0.27354\n",
      "batch 27: running loss = 2.06651, running accuracy = 0.27511\n",
      "batch 28: running loss = 2.06291, running accuracy = 0.27629\n",
      "batch 29: running loss = 2.05872, running accuracy = 0.27959\n",
      "batch 30: running loss = 2.05566, running accuracy = 0.28120\n",
      "batch 31: running loss = 2.05305, running accuracy = 0.28245\n",
      "batch 32: running loss = 2.05060, running accuracy = 0.28350\n",
      "batch 33: running loss = 2.04683, running accuracy = 0.28582\n",
      "batch 34: running loss = 2.04510, running accuracy = 0.28765\n",
      "batch 35: running loss = 2.04137, running accuracy = 0.28937\n",
      "batch 36: running loss = 2.04051, running accuracy = 0.28956\n",
      "batch 37: running loss = 2.03847, running accuracy = 0.29049\n",
      "batch 38: running loss = 2.03639, running accuracy = 0.29105\n",
      "batch 39: running loss = 2.03310, running accuracy = 0.29251\n",
      "batch 40: running loss = 2.03061, running accuracy = 0.29510\n",
      "batch 41: running loss = 2.03001, running accuracy = 0.29639\n",
      "batch 42: running loss = 2.02800, running accuracy = 0.29705\n",
      "batch 43: running loss = 2.02579, running accuracy = 0.29693\n",
      "batch 44: running loss = 2.02345, running accuracy = 0.29791\n",
      "batch 45: running loss = 2.02143, running accuracy = 0.29867\n",
      "batch 46: running loss = 2.01989, running accuracy = 0.30000\n",
      "batch 47: running loss = 2.01731, running accuracy = 0.30153\n",
      "batch 48: running loss = 2.01641, running accuracy = 0.30208\n",
      "batch 49: running loss = 2.01428, running accuracy = 0.30237\n",
      "batch 50: running loss = 2.01237, running accuracy = 0.30416\n",
      "batch 51: running loss = 2.01016, running accuracy = 0.30478\n",
      "batch 52: running loss = 2.00851, running accuracy = 0.30492\n",
      "batch 53: running loss = 2.00746, running accuracy = 0.30528\n",
      "batch 54: running loss = 2.00607, running accuracy = 0.30548\n",
      "batch 55: running loss = 2.00347, running accuracy = 0.30655\n",
      "batch 56: running loss = 2.00115, running accuracy = 0.30700\n",
      "batch 57: running loss = 1.99939, running accuracy = 0.30716\n",
      "batch 58: running loss = 1.99925, running accuracy = 0.30772\n",
      "batch 59: running loss = 1.99782, running accuracy = 0.30942\n",
      "batch 60: running loss = 1.99629, running accuracy = 0.31027\n",
      "batch 61: running loss = 1.99551, running accuracy = 0.31036\n",
      "batch 62: running loss = 1.99442, running accuracy = 0.31026\n",
      "batch 63: running loss = 1.99307, running accuracy = 0.31092\n",
      "batch 64: running loss = 1.99251, running accuracy = 0.31100\n",
      "batch 65: running loss = 1.99099, running accuracy = 0.31151\n",
      "batch 66: running loss = 1.98940, running accuracy = 0.31170\n",
      "batch 67: running loss = 1.98876, running accuracy = 0.31164\n",
      "batch 68: running loss = 1.98691, running accuracy = 0.31194\n",
      "batch 69: running loss = 1.98617, running accuracy = 0.31258\n",
      "batch 70: running loss = 1.98513, running accuracy = 0.31320\n",
      "batch 71: running loss = 1.98367, running accuracy = 0.31352\n",
      "batch 72: running loss = 1.98132, running accuracy = 0.31489\n",
      "batch 73: running loss = 1.97982, running accuracy = 0.31556\n",
      "batch 74: running loss = 1.97845, running accuracy = 0.31605\n",
      "batch 75: running loss = 1.97765, running accuracy = 0.31653\n",
      "batch 76: running loss = 1.97638, running accuracy = 0.31768\n",
      "batch 77: running loss = 1.97508, running accuracy = 0.31792\n",
      "batch 78: running loss = 1.97387, running accuracy = 0.31826\n",
      "batch 79: running loss = 1.97267, running accuracy = 0.31868\n",
      "batch 80: running loss = 1.97142, running accuracy = 0.31960\n",
      "batch 81: running loss = 1.97035, running accuracy = 0.32049\n",
      "batch 82: running loss = 1.96923, running accuracy = 0.32117\n",
      "batch 83: running loss = 1.96865, running accuracy = 0.32164\n",
      "batch 84: running loss = 1.96653, running accuracy = 0.32229\n",
      "batch 85: running loss = 1.96587, running accuracy = 0.32268\n",
      "batch 86: running loss = 1.96464, running accuracy = 0.32349\n",
      "batch 87: running loss = 1.96240, running accuracy = 0.32460\n",
      "batch 88: running loss = 1.96210, running accuracy = 0.32459\n",
      "batch 89: running loss = 1.96114, running accuracy = 0.32472\n",
      "batch 90: running loss = 1.95987, running accuracy = 0.32529\n",
      "batch 91: running loss = 1.95824, running accuracy = 0.32651\n",
      "batch 92: running loss = 1.95742, running accuracy = 0.32670\n",
      "batch 93: running loss = 1.95618, running accuracy = 0.32714\n",
      "batch 94: running loss = 1.95403, running accuracy = 0.32813\n",
      "batch 95: running loss = 1.95167, running accuracy = 0.32931\n",
      "batch 96: running loss = 1.95116, running accuracy = 0.32917\n",
      "batch 97: running loss = 1.94989, running accuracy = 0.32973\n",
      "batch 98: running loss = 1.94838, running accuracy = 0.33024\n",
      "batch 99: running loss = 1.94737, running accuracy = 0.33034\n",
      "batch 100: running loss = 1.94657, running accuracy = 0.33080\n",
      "batch 101: running loss = 1.94522, running accuracy = 0.33160\n",
      "batch 102: running loss = 1.94421, running accuracy = 0.33161\n",
      "batch 103: running loss = 1.94389, running accuracy = 0.33239\n",
      "batch 104: running loss = 1.94289, running accuracy = 0.33300\n",
      "batch 105: running loss = 1.94068, running accuracy = 0.33394\n",
      "batch 106: running loss = 1.93985, running accuracy = 0.33434\n",
      "batch 107: running loss = 1.93886, running accuracy = 0.33454\n",
      "batch 108: running loss = 1.93820, running accuracy = 0.33489\n",
      "batch 109: running loss = 1.93767, running accuracy = 0.33519\n",
      "batch 110: running loss = 1.93730, running accuracy = 0.33542\n",
      "batch 111: running loss = 1.93616, running accuracy = 0.33564\n",
      "batch 112: running loss = 1.93530, running accuracy = 0.33621\n",
      "batch 113: running loss = 1.93542, running accuracy = 0.33618\n",
      "batch 114: running loss = 1.93447, running accuracy = 0.33656\n",
      "batch 115: running loss = 1.93387, running accuracy = 0.33666\n",
      "batch 116: running loss = 1.93225, running accuracy = 0.33755\n",
      "batch 117: running loss = 1.93193, running accuracy = 0.33764\n",
      "batch 118: running loss = 1.93084, running accuracy = 0.33793\n",
      "batch 119: running loss = 1.93034, running accuracy = 0.33778\n",
      "batch 120: running loss = 1.92916, running accuracy = 0.33810\n",
      "batch 121: running loss = 1.92860, running accuracy = 0.33825\n",
      "batch 122: running loss = 1.92791, running accuracy = 0.33839\n",
      "batch 123: running loss = 1.92796, running accuracy = 0.33876\n",
      "batch 124: running loss = 1.92697, running accuracy = 0.33910\n",
      "batch 125: running loss = 1.92681, running accuracy = 0.33936\n",
      "batch 126: running loss = 1.92626, running accuracy = 0.33978\n",
      "batch 127: running loss = 1.92540, running accuracy = 0.33994\n",
      "batch 128: running loss = 1.92522, running accuracy = 0.34028\n",
      "batch 129: running loss = 1.92471, running accuracy = 0.34047\n",
      "batch 130: running loss = 1.92357, running accuracy = 0.34089\n",
      "batch 131: running loss = 1.92322, running accuracy = 0.34085\n",
      "batch 132: running loss = 1.92307, running accuracy = 0.34091\n",
      "batch 133: running loss = 1.92227, running accuracy = 0.34111\n",
      "batch 134: running loss = 1.92189, running accuracy = 0.34110\n",
      "batch 135: running loss = 1.92222, running accuracy = 0.34101\n",
      "batch 136: running loss = 1.92126, running accuracy = 0.34121\n",
      "batch 137: running loss = 1.92048, running accuracy = 0.34161\n",
      "batch 138: running loss = 1.91986, running accuracy = 0.34200\n",
      "batch 139: running loss = 1.91908, running accuracy = 0.34247\n",
      "batch 140: running loss = 1.91840, running accuracy = 0.34280\n",
      "batch 141: running loss = 1.91803, running accuracy = 0.34304\n",
      "batch 142: running loss = 1.91772, running accuracy = 0.34321\n",
      "batch 143: running loss = 1.91718, running accuracy = 0.34336\n",
      "batch 144: running loss = 1.91658, running accuracy = 0.34342\n",
      "batch 145: running loss = 1.91629, running accuracy = 0.34370\n",
      "batch 146: running loss = 1.91551, running accuracy = 0.34411\n",
      "batch 147: running loss = 1.91461, running accuracy = 0.34438\n",
      "batch 148: running loss = 1.91409, running accuracy = 0.34443\n",
      "batch 149: running loss = 1.91334, running accuracy = 0.34440\n",
      "batch 150: running loss = 1.91293, running accuracy = 0.34456\n",
      "batch 151: running loss = 1.91267, running accuracy = 0.34461\n",
      "batch 152: running loss = 1.91248, running accuracy = 0.34471\n",
      "batch 153: running loss = 1.91233, running accuracy = 0.34478\n",
      "batch 154: running loss = 1.91200, running accuracy = 0.34491\n",
      "batch 155: running loss = 1.91137, running accuracy = 0.34508\n",
      "batch 156: running loss = 1.91029, running accuracy = 0.34559\n",
      "batch 157: running loss = 1.90969, running accuracy = 0.34561\n",
      "batch 158: running loss = 1.90926, running accuracy = 0.34567\n",
      "batch 159: running loss = 1.90904, running accuracy = 0.34611\n",
      "batch 160: running loss = 1.90886, running accuracy = 0.34645\n",
      "batch 161: running loss = 1.90858, running accuracy = 0.34651\n",
      "batch 162: running loss = 1.90838, running accuracy = 0.34684\n",
      "batch 163: running loss = 1.90748, running accuracy = 0.34719\n",
      "batch 164: running loss = 1.90771, running accuracy = 0.34717\n",
      "batch 165: running loss = 1.90761, running accuracy = 0.34703\n",
      "batch 166: running loss = 1.90746, running accuracy = 0.34682\n",
      "batch 167: running loss = 1.90699, running accuracy = 0.34699\n",
      "batch 168: running loss = 1.90605, running accuracy = 0.34714\n",
      "batch 169: running loss = 1.90567, running accuracy = 0.34703\n",
      "batch 170: running loss = 1.90464, running accuracy = 0.34762\n",
      "batch 171: running loss = 1.90460, running accuracy = 0.34718\n",
      "batch 172: running loss = 1.90441, running accuracy = 0.34714\n",
      "batch 173: running loss = 1.90402, running accuracy = 0.34731\n",
      "batch 174: running loss = 1.90355, running accuracy = 0.34756\n",
      "batch 175: running loss = 1.90270, running accuracy = 0.34766\n",
      "batch 176: running loss = 1.90218, running accuracy = 0.34755\n",
      "batch 177: running loss = 1.90173, running accuracy = 0.34768\n",
      "batch 178: running loss = 1.90178, running accuracy = 0.34773\n",
      "batch 179: running loss = 1.90129, running accuracy = 0.34775\n",
      "batch 180: running loss = 1.90090, running accuracy = 0.34796\n",
      "batch 181: running loss = 1.90041, running accuracy = 0.34831\n",
      "batch 182: running loss = 1.90023, running accuracy = 0.34826\n",
      "batch 183: running loss = 1.89961, running accuracy = 0.34835\n",
      "batch 184: running loss = 1.89928, running accuracy = 0.34826\n",
      "batch 185: running loss = 1.89839, running accuracy = 0.34858\n",
      "batch 186: running loss = 1.89828, running accuracy = 0.34858\n",
      "batch 187: running loss = 1.89801, running accuracy = 0.34860\n",
      "batch 188: running loss = 1.89755, running accuracy = 0.34909\n",
      "batch 189: running loss = 1.89743, running accuracy = 0.34927\n",
      "batch 190: running loss = 1.89691, running accuracy = 0.34941\n",
      "batch 191: running loss = 1.89611, running accuracy = 0.34970\n",
      "batch 192: running loss = 1.89589, running accuracy = 0.34973\n",
      "batch 193: running loss = 1.89608, running accuracy = 0.34951\n",
      "batch 194: running loss = 1.89567, running accuracy = 0.34951\n",
      "batch 195: running loss = 1.89511, running accuracy = 0.34960\n",
      "batch 196: running loss = 1.89483, running accuracy = 0.34986\n",
      "batch 197: running loss = 1.89402, running accuracy = 0.35031\n",
      "batch 198: running loss = 1.89372, running accuracy = 0.35055\n",
      "batch 199: running loss = 1.89372, running accuracy = 0.35057\n",
      "epoch 2\n",
      "batch 1: running loss = 1.79446, running accuracy = 0.39200\n",
      "batch 2: running loss = 1.76429, running accuracy = 0.38800\n",
      "batch 3: running loss = 1.76376, running accuracy = 0.38667\n",
      "batch 4: running loss = 1.77587, running accuracy = 0.38500\n",
      "batch 5: running loss = 1.76666, running accuracy = 0.39280\n",
      "batch 6: running loss = 1.76168, running accuracy = 0.39267\n",
      "batch 7: running loss = 1.76351, running accuracy = 0.39314\n",
      "batch 8: running loss = 1.77370, running accuracy = 0.39400\n",
      "batch 9: running loss = 1.78205, running accuracy = 0.39422\n",
      "batch 10: running loss = 1.78024, running accuracy = 0.39440\n",
      "batch 11: running loss = 1.78621, running accuracy = 0.39127\n",
      "batch 12: running loss = 1.78713, running accuracy = 0.39033\n",
      "batch 13: running loss = 1.78949, running accuracy = 0.38862\n",
      "batch 14: running loss = 1.79047, running accuracy = 0.38829\n",
      "batch 15: running loss = 1.78761, running accuracy = 0.38933\n",
      "batch 16: running loss = 1.78944, running accuracy = 0.38975\n",
      "batch 17: running loss = 1.78759, running accuracy = 0.39176\n",
      "batch 18: running loss = 1.79367, running accuracy = 0.38844\n",
      "batch 19: running loss = 1.79122, running accuracy = 0.39074\n",
      "batch 20: running loss = 1.79289, running accuracy = 0.39140\n",
      "batch 21: running loss = 1.79991, running accuracy = 0.38914\n",
      "batch 22: running loss = 1.79839, running accuracy = 0.39036\n",
      "batch 23: running loss = 1.79596, running accuracy = 0.38974\n",
      "batch 24: running loss = 1.79366, running accuracy = 0.39100\n",
      "batch 25: running loss = 1.79065, running accuracy = 0.39168\n",
      "batch 26: running loss = 1.79113, running accuracy = 0.39262\n",
      "batch 27: running loss = 1.79212, running accuracy = 0.39185\n",
      "batch 28: running loss = 1.79072, running accuracy = 0.39214\n",
      "batch 29: running loss = 1.78889, running accuracy = 0.39352\n",
      "batch 30: running loss = 1.78806, running accuracy = 0.39453\n",
      "batch 31: running loss = 1.78961, running accuracy = 0.39381\n",
      "batch 32: running loss = 1.79069, running accuracy = 0.39300\n",
      "batch 33: running loss = 1.78919, running accuracy = 0.39382\n",
      "batch 34: running loss = 1.79155, running accuracy = 0.39341\n",
      "batch 35: running loss = 1.78947, running accuracy = 0.39486\n",
      "batch 36: running loss = 1.79284, running accuracy = 0.39344\n",
      "batch 37: running loss = 1.79396, running accuracy = 0.39265\n",
      "batch 38: running loss = 1.79447, running accuracy = 0.39211\n",
      "batch 39: running loss = 1.79319, running accuracy = 0.39303\n",
      "batch 40: running loss = 1.79296, running accuracy = 0.39380\n",
      "batch 41: running loss = 1.79581, running accuracy = 0.39327\n",
      "batch 42: running loss = 1.79677, running accuracy = 0.39257\n",
      "batch 43: running loss = 1.79666, running accuracy = 0.39200\n",
      "batch 44: running loss = 1.79702, running accuracy = 0.39209\n",
      "batch 45: running loss = 1.79731, running accuracy = 0.39218\n",
      "batch 46: running loss = 1.79775, running accuracy = 0.39209\n",
      "batch 47: running loss = 1.79695, running accuracy = 0.39302\n",
      "batch 48: running loss = 1.79829, running accuracy = 0.39225\n",
      "batch 49: running loss = 1.79842, running accuracy = 0.39127\n",
      "batch 50: running loss = 1.79968, running accuracy = 0.39168\n",
      "batch 51: running loss = 1.79931, running accuracy = 0.39106\n",
      "batch 52: running loss = 1.79879, running accuracy = 0.39085\n",
      "batch 53: running loss = 1.80025, running accuracy = 0.38974\n",
      "batch 54: running loss = 1.80099, running accuracy = 0.38970\n",
      "batch 55: running loss = 1.79976, running accuracy = 0.38931\n",
      "batch 56: running loss = 1.79893, running accuracy = 0.38936\n",
      "batch 57: running loss = 1.79876, running accuracy = 0.38884\n",
      "batch 58: running loss = 1.80040, running accuracy = 0.38883\n",
      "batch 59: running loss = 1.80052, running accuracy = 0.38956\n",
      "batch 60: running loss = 1.80087, running accuracy = 0.38987\n",
      "batch 61: running loss = 1.80216, running accuracy = 0.38925\n",
      "batch 62: running loss = 1.80233, running accuracy = 0.38877\n",
      "batch 63: running loss = 1.80246, running accuracy = 0.38844\n",
      "batch 64: running loss = 1.80393, running accuracy = 0.38787\n",
      "batch 65: running loss = 1.80351, running accuracy = 0.38794\n",
      "batch 66: running loss = 1.80317, running accuracy = 0.38758\n",
      "batch 67: running loss = 1.80423, running accuracy = 0.38746\n",
      "batch 68: running loss = 1.80308, running accuracy = 0.38729\n",
      "batch 69: running loss = 1.80356, running accuracy = 0.38783\n",
      "batch 70: running loss = 1.80376, running accuracy = 0.38777\n",
      "batch 71: running loss = 1.80354, running accuracy = 0.38772\n",
      "batch 72: running loss = 1.80238, running accuracy = 0.38850\n",
      "batch 73: running loss = 1.80201, running accuracy = 0.38882\n",
      "batch 74: running loss = 1.80165, running accuracy = 0.38881\n",
      "batch 75: running loss = 1.80220, running accuracy = 0.38880\n",
      "batch 76: running loss = 1.80188, running accuracy = 0.38958\n",
      "batch 77: running loss = 1.80156, running accuracy = 0.38966\n",
      "batch 78: running loss = 1.80105, running accuracy = 0.38944\n",
      "batch 79: running loss = 1.80141, running accuracy = 0.38942\n",
      "batch 80: running loss = 1.80115, running accuracy = 0.38935\n",
      "batch 81: running loss = 1.80129, running accuracy = 0.38909\n",
      "batch 82: running loss = 1.80118, running accuracy = 0.38927\n",
      "batch 83: running loss = 1.80196, running accuracy = 0.38887\n",
      "batch 84: running loss = 1.80059, running accuracy = 0.38957\n",
      "batch 85: running loss = 1.80075, running accuracy = 0.38969\n",
      "batch 86: running loss = 1.80045, running accuracy = 0.39051\n",
      "batch 87: running loss = 1.79888, running accuracy = 0.39145\n",
      "batch 88: running loss = 1.79968, running accuracy = 0.39086\n",
      "batch 89: running loss = 1.79971, running accuracy = 0.39020\n",
      "batch 90: running loss = 1.79944, running accuracy = 0.38991\n",
      "batch 91: running loss = 1.79866, running accuracy = 0.39055\n",
      "batch 92: running loss = 1.79851, running accuracy = 0.39070\n",
      "batch 93: running loss = 1.79804, running accuracy = 0.39041\n",
      "batch 94: running loss = 1.79647, running accuracy = 0.39106\n",
      "batch 95: running loss = 1.79491, running accuracy = 0.39175\n",
      "batch 96: running loss = 1.79527, running accuracy = 0.39137\n",
      "batch 97: running loss = 1.79469, running accuracy = 0.39192\n",
      "batch 98: running loss = 1.79399, running accuracy = 0.39245\n",
      "batch 99: running loss = 1.79372, running accuracy = 0.39212\n",
      "batch 100: running loss = 1.79367, running accuracy = 0.39224\n",
      "batch 101: running loss = 1.79316, running accuracy = 0.39271\n",
      "batch 102: running loss = 1.79293, running accuracy = 0.39263\n",
      "batch 103: running loss = 1.79337, running accuracy = 0.39301\n",
      "batch 104: running loss = 1.79308, running accuracy = 0.39315\n",
      "batch 105: running loss = 1.79144, running accuracy = 0.39379\n",
      "batch 106: running loss = 1.79121, running accuracy = 0.39381\n",
      "batch 107: running loss = 1.79092, running accuracy = 0.39376\n",
      "batch 108: running loss = 1.79102, running accuracy = 0.39367\n",
      "batch 109: running loss = 1.79111, running accuracy = 0.39369\n",
      "batch 110: running loss = 1.79146, running accuracy = 0.39360\n",
      "batch 111: running loss = 1.79107, running accuracy = 0.39326\n",
      "batch 112: running loss = 1.79104, running accuracy = 0.39339\n",
      "batch 113: running loss = 1.79198, running accuracy = 0.39288\n",
      "batch 114: running loss = 1.79168, running accuracy = 0.39274\n",
      "batch 115: running loss = 1.79172, running accuracy = 0.39259\n",
      "batch 116: running loss = 1.79060, running accuracy = 0.39317\n",
      "batch 117: running loss = 1.79104, running accuracy = 0.39296\n",
      "batch 118: running loss = 1.79055, running accuracy = 0.39319\n",
      "batch 119: running loss = 1.79084, running accuracy = 0.39297\n",
      "batch 120: running loss = 1.79029, running accuracy = 0.39310\n",
      "batch 121: running loss = 1.79025, running accuracy = 0.39296\n",
      "batch 122: running loss = 1.79012, running accuracy = 0.39285\n",
      "batch 123: running loss = 1.79076, running accuracy = 0.39278\n",
      "batch 124: running loss = 1.79032, running accuracy = 0.39303\n",
      "batch 125: running loss = 1.79075, running accuracy = 0.39280\n",
      "batch 126: running loss = 1.79083, running accuracy = 0.39286\n",
      "batch 127: running loss = 1.79053, running accuracy = 0.39288\n",
      "batch 128: running loss = 1.79097, running accuracy = 0.39313\n",
      "batch 129: running loss = 1.79102, running accuracy = 0.39302\n",
      "batch 130: running loss = 1.79042, running accuracy = 0.39323\n",
      "batch 131: running loss = 1.79056, running accuracy = 0.39301\n",
      "batch 132: running loss = 1.79098, running accuracy = 0.39294\n",
      "batch 133: running loss = 1.79075, running accuracy = 0.39293\n",
      "batch 134: running loss = 1.79083, running accuracy = 0.39269\n",
      "batch 135: running loss = 1.79192, running accuracy = 0.39233\n",
      "batch 136: running loss = 1.79144, running accuracy = 0.39253\n",
      "batch 137: running loss = 1.79110, running accuracy = 0.39273\n",
      "batch 138: running loss = 1.79113, running accuracy = 0.39287\n",
      "batch 139: running loss = 1.79093, running accuracy = 0.39309\n",
      "batch 140: running loss = 1.79078, running accuracy = 0.39329\n",
      "batch 141: running loss = 1.79089, running accuracy = 0.39316\n",
      "batch 142: running loss = 1.79097, running accuracy = 0.39327\n",
      "batch 143: running loss = 1.79095, running accuracy = 0.39315\n",
      "batch 144: running loss = 1.79087, running accuracy = 0.39311\n",
      "batch 145: running loss = 1.79108, running accuracy = 0.39324\n",
      "batch 146: running loss = 1.79073, running accuracy = 0.39356\n",
      "batch 147: running loss = 1.79032, running accuracy = 0.39361\n",
      "batch 148: running loss = 1.79030, running accuracy = 0.39349\n",
      "batch 149: running loss = 1.78991, running accuracy = 0.39345\n",
      "batch 150: running loss = 1.78982, running accuracy = 0.39339\n",
      "batch 151: running loss = 1.79010, running accuracy = 0.39314\n",
      "batch 152: running loss = 1.79024, running accuracy = 0.39305\n",
      "batch 153: running loss = 1.79059, running accuracy = 0.39299\n",
      "batch 154: running loss = 1.79073, running accuracy = 0.39286\n",
      "batch 155: running loss = 1.79052, running accuracy = 0.39283\n",
      "batch 156: running loss = 1.78979, running accuracy = 0.39321\n",
      "batch 157: running loss = 1.78960, running accuracy = 0.39307\n",
      "batch 158: running loss = 1.78957, running accuracy = 0.39296\n",
      "batch 159: running loss = 1.78979, running accuracy = 0.39313\n",
      "batch 160: running loss = 1.79003, running accuracy = 0.39320\n",
      "batch 161: running loss = 1.79015, running accuracy = 0.39317\n",
      "batch 162: running loss = 1.79045, running accuracy = 0.39336\n",
      "batch 163: running loss = 1.78997, running accuracy = 0.39345\n",
      "batch 164: running loss = 1.79072, running accuracy = 0.39315\n",
      "batch 165: running loss = 1.79104, running accuracy = 0.39273\n",
      "batch 166: running loss = 1.79140, running accuracy = 0.39243\n",
      "batch 167: running loss = 1.79140, running accuracy = 0.39255\n",
      "batch 168: running loss = 1.79082, running accuracy = 0.39250\n",
      "batch 169: running loss = 1.79080, running accuracy = 0.39231\n",
      "batch 170: running loss = 1.79010, running accuracy = 0.39268\n",
      "batch 171: running loss = 1.79050, running accuracy = 0.39235\n",
      "batch 172: running loss = 1.79067, running accuracy = 0.39228\n",
      "batch 173: running loss = 1.79070, running accuracy = 0.39223\n",
      "batch 174: running loss = 1.79061, running accuracy = 0.39230\n",
      "batch 175: running loss = 1.79001, running accuracy = 0.39243\n",
      "batch 176: running loss = 1.78996, running accuracy = 0.39216\n",
      "batch 177: running loss = 1.78996, running accuracy = 0.39214\n",
      "batch 178: running loss = 1.79040, running accuracy = 0.39198\n",
      "batch 179: running loss = 1.79027, running accuracy = 0.39182\n",
      "batch 180: running loss = 1.79022, running accuracy = 0.39182\n",
      "batch 181: running loss = 1.79011, running accuracy = 0.39209\n",
      "batch 182: running loss = 1.79024, running accuracy = 0.39196\n",
      "batch 183: running loss = 1.78994, running accuracy = 0.39189\n",
      "batch 184: running loss = 1.78993, running accuracy = 0.39161\n",
      "batch 185: running loss = 1.78936, running accuracy = 0.39194\n",
      "batch 186: running loss = 1.78964, running accuracy = 0.39172\n",
      "batch 187: running loss = 1.78966, running accuracy = 0.39166\n",
      "batch 188: running loss = 1.78946, running accuracy = 0.39209\n",
      "batch 189: running loss = 1.78974, running accuracy = 0.39211\n",
      "batch 190: running loss = 1.78956, running accuracy = 0.39213\n",
      "batch 191: running loss = 1.78906, running accuracy = 0.39231\n",
      "batch 192: running loss = 1.78919, running accuracy = 0.39223\n",
      "batch 193: running loss = 1.78977, running accuracy = 0.39183\n",
      "batch 194: running loss = 1.78967, running accuracy = 0.39181\n",
      "batch 195: running loss = 1.78949, running accuracy = 0.39177\n",
      "batch 196: running loss = 1.78956, running accuracy = 0.39188\n",
      "batch 197: running loss = 1.78906, running accuracy = 0.39210\n",
      "batch 198: running loss = 1.78910, running accuracy = 0.39208\n",
      "batch 199: running loss = 1.78943, running accuracy = 0.39202\n",
      "epoch 3\n",
      "batch 1: running loss = 1.74546, running accuracy = 0.38800\n",
      "batch 2: running loss = 1.71935, running accuracy = 0.39000\n",
      "batch 3: running loss = 1.72027, running accuracy = 0.39333\n",
      "batch 4: running loss = 1.73252, running accuracy = 0.39900\n",
      "batch 5: running loss = 1.72362, running accuracy = 0.40400\n",
      "batch 6: running loss = 1.71769, running accuracy = 0.40533\n",
      "batch 7: running loss = 1.71947, running accuracy = 0.40629\n",
      "batch 8: running loss = 1.72631, running accuracy = 0.41200\n",
      "batch 9: running loss = 1.73527, running accuracy = 0.40933\n",
      "batch 10: running loss = 1.73388, running accuracy = 0.40720\n",
      "batch 11: running loss = 1.74067, running accuracy = 0.40400\n",
      "batch 12: running loss = 1.74179, running accuracy = 0.40200\n",
      "batch 13: running loss = 1.74540, running accuracy = 0.40000\n",
      "batch 14: running loss = 1.74641, running accuracy = 0.39943\n",
      "batch 15: running loss = 1.74379, running accuracy = 0.39973\n",
      "batch 16: running loss = 1.74594, running accuracy = 0.39975\n",
      "batch 17: running loss = 1.74470, running accuracy = 0.40141\n",
      "batch 18: running loss = 1.75118, running accuracy = 0.39800\n",
      "batch 19: running loss = 1.74887, running accuracy = 0.40021\n",
      "batch 20: running loss = 1.75089, running accuracy = 0.40000\n",
      "batch 21: running loss = 1.75854, running accuracy = 0.39771\n",
      "batch 22: running loss = 1.75708, running accuracy = 0.39891\n",
      "batch 23: running loss = 1.75471, running accuracy = 0.39913\n",
      "batch 24: running loss = 1.75243, running accuracy = 0.40050\n",
      "batch 25: running loss = 1.74915, running accuracy = 0.40144\n",
      "batch 26: running loss = 1.74914, running accuracy = 0.40231\n",
      "batch 27: running loss = 1.75008, running accuracy = 0.40178\n",
      "batch 28: running loss = 1.74883, running accuracy = 0.40171\n",
      "batch 29: running loss = 1.74675, running accuracy = 0.40317\n",
      "batch 30: running loss = 1.74577, running accuracy = 0.40387\n",
      "batch 31: running loss = 1.74767, running accuracy = 0.40323\n",
      "batch 32: running loss = 1.74893, running accuracy = 0.40287\n",
      "batch 33: running loss = 1.74742, running accuracy = 0.40339\n",
      "batch 34: running loss = 1.75011, running accuracy = 0.40329\n",
      "batch 35: running loss = 1.74798, running accuracy = 0.40457\n",
      "batch 36: running loss = 1.75162, running accuracy = 0.40344\n",
      "batch 37: running loss = 1.75299, running accuracy = 0.40303\n",
      "batch 38: running loss = 1.75372, running accuracy = 0.40211\n",
      "batch 39: running loss = 1.75247, running accuracy = 0.40338\n",
      "batch 40: running loss = 1.75220, running accuracy = 0.40420\n",
      "batch 41: running loss = 1.75523, running accuracy = 0.40390\n",
      "batch 42: running loss = 1.75640, running accuracy = 0.40343\n",
      "batch 43: running loss = 1.75626, running accuracy = 0.40260\n",
      "batch 44: running loss = 1.75670, running accuracy = 0.40291\n",
      "batch 45: running loss = 1.75709, running accuracy = 0.40293\n",
      "batch 46: running loss = 1.75772, running accuracy = 0.40296\n",
      "batch 47: running loss = 1.75681, running accuracy = 0.40366\n",
      "batch 48: running loss = 1.75834, running accuracy = 0.40292\n",
      "batch 49: running loss = 1.75882, running accuracy = 0.40237\n",
      "batch 50: running loss = 1.76061, running accuracy = 0.40248\n",
      "batch 51: running loss = 1.76044, running accuracy = 0.40173\n",
      "batch 52: running loss = 1.75996, running accuracy = 0.40177\n",
      "batch 53: running loss = 1.76179, running accuracy = 0.40083\n",
      "batch 54: running loss = 1.76265, running accuracy = 0.40081\n",
      "batch 55: running loss = 1.76136, running accuracy = 0.40065\n",
      "batch 56: running loss = 1.76058, running accuracy = 0.40107\n",
      "batch 57: running loss = 1.76054, running accuracy = 0.40084\n",
      "batch 58: running loss = 1.76231, running accuracy = 0.40069\n",
      "batch 59: running loss = 1.76237, running accuracy = 0.40115\n",
      "batch 60: running loss = 1.76293, running accuracy = 0.40140\n",
      "batch 61: running loss = 1.76447, running accuracy = 0.40079\n",
      "batch 62: running loss = 1.76465, running accuracy = 0.40032\n",
      "batch 63: running loss = 1.76487, running accuracy = 0.40032\n",
      "batch 64: running loss = 1.76659, running accuracy = 0.39975\n",
      "batch 65: running loss = 1.76617, running accuracy = 0.40018\n",
      "batch 66: running loss = 1.76577, running accuracy = 0.39970\n",
      "batch 67: running loss = 1.76692, running accuracy = 0.39952\n",
      "batch 68: running loss = 1.76567, running accuracy = 0.39947\n",
      "batch 69: running loss = 1.76616, running accuracy = 0.39994\n",
      "batch 70: running loss = 1.76661, running accuracy = 0.40000\n",
      "batch 71: running loss = 1.76656, running accuracy = 0.40000\n",
      "batch 72: running loss = 1.76552, running accuracy = 0.40056\n",
      "batch 73: running loss = 1.76526, running accuracy = 0.40082\n",
      "batch 74: running loss = 1.76496, running accuracy = 0.40103\n",
      "batch 75: running loss = 1.76559, running accuracy = 0.40096\n",
      "batch 76: running loss = 1.76524, running accuracy = 0.40195\n",
      "batch 77: running loss = 1.76500, running accuracy = 0.40161\n",
      "batch 78: running loss = 1.76440, running accuracy = 0.40159\n",
      "batch 79: running loss = 1.76503, running accuracy = 0.40111\n",
      "batch 80: running loss = 1.76474, running accuracy = 0.40120\n",
      "batch 81: running loss = 1.76509, running accuracy = 0.40064\n",
      "batch 82: running loss = 1.76503, running accuracy = 0.40054\n",
      "batch 83: running loss = 1.76593, running accuracy = 0.40010\n",
      "batch 84: running loss = 1.76457, running accuracy = 0.40062\n",
      "batch 85: running loss = 1.76468, running accuracy = 0.40104\n",
      "batch 86: running loss = 1.76440, running accuracy = 0.40153\n",
      "batch 87: running loss = 1.76279, running accuracy = 0.40248\n",
      "batch 88: running loss = 1.76370, running accuracy = 0.40200\n",
      "batch 89: running loss = 1.76383, running accuracy = 0.40139\n",
      "batch 90: running loss = 1.76369, running accuracy = 0.40093\n",
      "batch 91: running loss = 1.76299, running accuracy = 0.40141\n",
      "batch 92: running loss = 1.76280, running accuracy = 0.40161\n",
      "batch 93: running loss = 1.76233, running accuracy = 0.40168\n",
      "batch 94: running loss = 1.76076, running accuracy = 0.40209\n",
      "batch 95: running loss = 1.75921, running accuracy = 0.40291\n",
      "batch 96: running loss = 1.75961, running accuracy = 0.40258\n",
      "batch 97: running loss = 1.75899, running accuracy = 0.40309\n",
      "batch 98: running loss = 1.75837, running accuracy = 0.40355\n",
      "batch 99: running loss = 1.75809, running accuracy = 0.40319\n",
      "batch 100: running loss = 1.75806, running accuracy = 0.40332\n",
      "batch 101: running loss = 1.75758, running accuracy = 0.40364\n",
      "batch 102: running loss = 1.75747, running accuracy = 0.40353\n",
      "batch 103: running loss = 1.75792, running accuracy = 0.40373\n",
      "batch 104: running loss = 1.75769, running accuracy = 0.40381\n",
      "batch 105: running loss = 1.75609, running accuracy = 0.40434\n",
      "batch 106: running loss = 1.75588, running accuracy = 0.40434\n",
      "batch 107: running loss = 1.75565, running accuracy = 0.40419\n",
      "batch 108: running loss = 1.75580, running accuracy = 0.40407\n",
      "batch 109: running loss = 1.75593, running accuracy = 0.40396\n",
      "batch 110: running loss = 1.75632, running accuracy = 0.40382\n",
      "batch 111: running loss = 1.75605, running accuracy = 0.40350\n",
      "batch 112: running loss = 1.75615, running accuracy = 0.40379\n",
      "batch 113: running loss = 1.75721, running accuracy = 0.40315\n",
      "batch 114: running loss = 1.75694, running accuracy = 0.40295\n",
      "batch 115: running loss = 1.75702, running accuracy = 0.40268\n",
      "batch 116: running loss = 1.75593, running accuracy = 0.40321\n",
      "batch 117: running loss = 1.75648, running accuracy = 0.40311\n",
      "batch 118: running loss = 1.75602, running accuracy = 0.40322\n",
      "batch 119: running loss = 1.75643, running accuracy = 0.40292\n",
      "batch 120: running loss = 1.75593, running accuracy = 0.40293\n",
      "batch 121: running loss = 1.75593, running accuracy = 0.40281\n",
      "batch 122: running loss = 1.75583, running accuracy = 0.40269\n",
      "batch 123: running loss = 1.75648, running accuracy = 0.40260\n",
      "batch 124: running loss = 1.75609, running accuracy = 0.40287\n",
      "batch 125: running loss = 1.75651, running accuracy = 0.40272\n",
      "batch 126: running loss = 1.75666, running accuracy = 0.40283\n",
      "batch 127: running loss = 1.75639, running accuracy = 0.40290\n",
      "batch 128: running loss = 1.75688, running accuracy = 0.40313\n",
      "batch 129: running loss = 1.75697, running accuracy = 0.40310\n",
      "batch 130: running loss = 1.75642, running accuracy = 0.40335\n",
      "batch 131: running loss = 1.75658, running accuracy = 0.40321\n",
      "batch 132: running loss = 1.75702, running accuracy = 0.40315\n",
      "batch 133: running loss = 1.75686, running accuracy = 0.40319\n",
      "batch 134: running loss = 1.75693, running accuracy = 0.40299\n",
      "batch 135: running loss = 1.75815, running accuracy = 0.40264\n",
      "batch 136: running loss = 1.75771, running accuracy = 0.40294\n",
      "batch 137: running loss = 1.75738, running accuracy = 0.40298\n",
      "batch 138: running loss = 1.75753, running accuracy = 0.40304\n",
      "batch 139: running loss = 1.75743, running accuracy = 0.40331\n",
      "batch 140: running loss = 1.75734, running accuracy = 0.40346\n",
      "batch 141: running loss = 1.75748, running accuracy = 0.40338\n",
      "batch 142: running loss = 1.75756, running accuracy = 0.40341\n",
      "batch 143: running loss = 1.75759, running accuracy = 0.40327\n",
      "batch 144: running loss = 1.75757, running accuracy = 0.40328\n",
      "batch 145: running loss = 1.75780, running accuracy = 0.40353\n",
      "batch 146: running loss = 1.75747, running accuracy = 0.40378\n",
      "batch 147: running loss = 1.75709, running accuracy = 0.40384\n",
      "batch 148: running loss = 1.75714, running accuracy = 0.40365\n",
      "batch 149: running loss = 1.75676, running accuracy = 0.40370\n",
      "batch 150: running loss = 1.75665, running accuracy = 0.40379\n",
      "batch 151: running loss = 1.75699, running accuracy = 0.40355\n",
      "batch 152: running loss = 1.75710, running accuracy = 0.40326\n",
      "batch 153: running loss = 1.75753, running accuracy = 0.40319\n",
      "batch 154: running loss = 1.75773, running accuracy = 0.40306\n",
      "batch 155: running loss = 1.75756, running accuracy = 0.40305\n",
      "batch 156: running loss = 1.75684, running accuracy = 0.40338\n",
      "batch 157: running loss = 1.75671, running accuracy = 0.40326\n",
      "batch 158: running loss = 1.75671, running accuracy = 0.40316\n",
      "batch 159: running loss = 1.75696, running accuracy = 0.40327\n",
      "batch 160: running loss = 1.75726, running accuracy = 0.40338\n",
      "batch 161: running loss = 1.75739, running accuracy = 0.40343\n",
      "batch 162: running loss = 1.75777, running accuracy = 0.40360\n",
      "batch 163: running loss = 1.75733, running accuracy = 0.40371\n",
      "batch 164: running loss = 1.75817, running accuracy = 0.40339\n",
      "batch 165: running loss = 1.75854, running accuracy = 0.40303\n",
      "batch 166: running loss = 1.75900, running accuracy = 0.40272\n",
      "batch 167: running loss = 1.75905, running accuracy = 0.40280\n",
      "batch 168: running loss = 1.75852, running accuracy = 0.40288\n",
      "batch 169: running loss = 1.75851, running accuracy = 0.40275\n",
      "batch 170: running loss = 1.75786, running accuracy = 0.40308\n",
      "batch 171: running loss = 1.75831, running accuracy = 0.40274\n",
      "batch 172: running loss = 1.75853, running accuracy = 0.40258\n",
      "batch 173: running loss = 1.75862, running accuracy = 0.40240\n",
      "batch 174: running loss = 1.75854, running accuracy = 0.40251\n",
      "batch 175: running loss = 1.75795, running accuracy = 0.40258\n",
      "batch 176: running loss = 1.75801, running accuracy = 0.40227\n",
      "batch 177: running loss = 1.75809, running accuracy = 0.40219\n",
      "batch 178: running loss = 1.75856, running accuracy = 0.40209\n",
      "batch 179: running loss = 1.75849, running accuracy = 0.40188\n",
      "batch 180: running loss = 1.75846, running accuracy = 0.40189\n",
      "batch 181: running loss = 1.75841, running accuracy = 0.40210\n",
      "batch 182: running loss = 1.75855, running accuracy = 0.40198\n",
      "batch 183: running loss = 1.75828, running accuracy = 0.40181\n",
      "batch 184: running loss = 1.75831, running accuracy = 0.40141\n",
      "batch 185: running loss = 1.75777, running accuracy = 0.40171\n",
      "batch 186: running loss = 1.75810, running accuracy = 0.40148\n",
      "batch 187: running loss = 1.75814, running accuracy = 0.40135\n",
      "batch 188: running loss = 1.75794, running accuracy = 0.40177\n",
      "batch 189: running loss = 1.75828, running accuracy = 0.40182\n",
      "batch 190: running loss = 1.75817, running accuracy = 0.40183\n",
      "batch 191: running loss = 1.75770, running accuracy = 0.40201\n",
      "batch 192: running loss = 1.75788, running accuracy = 0.40185\n",
      "batch 193: running loss = 1.75853, running accuracy = 0.40141\n",
      "batch 194: running loss = 1.75844, running accuracy = 0.40146\n",
      "batch 195: running loss = 1.75833, running accuracy = 0.40135\n",
      "batch 196: running loss = 1.75847, running accuracy = 0.40141\n",
      "batch 197: running loss = 1.75802, running accuracy = 0.40160\n",
      "batch 198: running loss = 1.75811, running accuracy = 0.40170\n",
      "batch 199: running loss = 1.75847, running accuracy = 0.40159\n"
     ]
    }
   ],
   "source": [
    "# code block to iterate over training data with mini-batches\n",
    "# make an instance of linear classifier\n",
    "n_dim = Xtr.shape[1]\n",
    "n_class = 10\n",
    "model = LinearClassifier(n_dim, n_class)\n",
    "\n",
    "batch_size = 250\n",
    "n_samples = Xtr.shape[0]\n",
    "running_loss = 0\n",
    "running_acc = 0\n",
    "log_loss = []\n",
    "log_acc = []\n",
    "\n",
    "n_epoch = 3\n",
    "\n",
    "for i in range(n_epoch):\n",
    "    print('epoch %d' %( i+1))\n",
    "    idx = 0\n",
    "    i = 1\n",
    "    while True:\n",
    "        if idx + batch_size >= n_samples:\n",
    "            break\n",
    "\n",
    "        batch_X = Xtr[idx: idx + batch_size]\n",
    "        batch_Y = Ytr[idx: idx + batch_size]\n",
    "\n",
    "        Y_pred = model.forward(batch_X)\n",
    "        Y_true = onehot(batch_Y, n_class)\n",
    "\n",
    "        batch_loss = CEloss(Y_true, Y_pred)\n",
    "        running_loss = ((i - 1) / i) * running_loss + (1 / i) * batch_loss\n",
    "        batch_acc = get_accuracy(batch_Y, Y_pred)\n",
    "        running_acc = ((i - 1) / i) * running_acc + (1 / i) * batch_acc\n",
    "\n",
    "        print('batch %d: running loss = %.5f, running accuracy = %.5f' %(i, running_loss, running_acc))\n",
    "        log_loss.append(running_loss)\n",
    "        log_acc.append(running_acc)\n",
    "\n",
    "        # 1) calculate gradient\n",
    "        gradW = eval_gradW(model, CEloss, batch_X, Y_true)\n",
    "        gradb = eval_gradb(model, CEloss, batch_X, Y_true)\n",
    "\n",
    "        # 2) update parameters\n",
    "        lr = 0.05\n",
    "        model.W = model.W - lr * gradW\n",
    "        model.b = model.b - lr * gradb\n",
    "\n",
    "        idx += batch_size\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instead of b, add ones to the end of input Xs, as dummy variables \n",
    "# ((f + h) - (f - h)) / 2h\n",
    "# vary h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f6d40253100>]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "_, ax = plt.subplots(1, 2, figsize=(10, 4))\n",
    "ax[0].plot(log_loss)\n",
    "ax[1].plot(log_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.749183021359016e+00"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test loss\n",
    "y_pred = model.forward(Xte)\n",
    "y_true = onehot(Yte, n_class)\n",
    "CEloss(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4001"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test accuracy\n",
    "get_accuracy(Yte, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.7403629740541118"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# training loss (for the entire set)\n",
    "y_pred = model.forward(Xtr)\n",
    "y_true = onehot(Ytr, n_class)\n",
    "CEloss(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.40936"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# training accuracy\n",
    "get_accuracy(Ytr, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d1a104cc5a30ffb61dcba35c048d59e2a2543d3b82917e7c4c5fa986c3cfbc1d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.11 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
